{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1709270201459}],"collapsed_sections":["FJNUwmbgGyua","mDgbUHAGgjLW","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","yiiVWRdJDDil","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - House sale data\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n","##### **Contribution**    - Individual/Team\n","##### **Team Member 1 -**Anubhav Bansal\n","##### **Team Member 2 -**Ansh Puri\n","##### **Team Member 3 -**Rohan Lakhanpal\n","##### **Team Member 4 -**Rishit"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["Predicting the sales price of houses is a critical task within the real estate market, directly impacting buyers, sellers, and industry professionals. This project aims to develop a predictive model capable of estimating the sale price for houses given a set of features or characteristics. The prediction will be based on a test set where each house is identified by an Id, and the goal is to forecast the value of the SalePrice variable accurately.\n","\n","Predicting the sale price of houses is a complex but valuable task that can significantly impact the real estate market. Through careful data preprocessing, thoughtful feature engineering, strategic model selection, and rigorous validation and evaluation, it is possible to develop a predictive model that can accurately estimate the sales price of houses, thereby aiding stakeholders in making informed decisions.\n","# > Indented block\n","\n"],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["https://github.com/anubhav9087/AIML"],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["**Write Problem Statement Here.**  The ultimate goal is to predict the sales price for each house in the test set, identified by its unique Id, thereby providing valuable insights for stakeholders in making informed decisions in the real estate market.\n","\n","\n","\n","\n"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# For data manipulation and analysis\n","import pandas as pd\n","import numpy as np\n","\n","# For data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set seaborn style for plots\n","sns.set()\n"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load the dataset\n","dataset_path = '/content/house_sale_data.csv'\n","df = pd.read_csv(dataset_path)\n","\n","# Display the first few rows of the dataframe\n","print(df.head())\n"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["print(df.head())\n","\n","# Print the size of the dataset (rows, columns)\n","print(f\"\\nThe dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["print(f\"The dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n","\n","\n","\n"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["print(df.info())\n"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["duplicate_count = df.duplicated().sum()\n","print(f\"There are {duplicate_count} duplicate rows in the dataset.\")"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["missing_values_count = df.isnull().sum()\n","\n","print(\"Missing Values Count:\")\n","print(missing_values_count)"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n","plt.title('Missing Values in the Dataset')\n","plt.show()\n","\n","\n","\n"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Size of the Dataset: The dataset contains a total of X rows and Y columns.\n","\n","Data Types: We have information about the data types of each column in the dataset, which includes numerical, categorical, and potentially datetime data types.\n","\n","Missing Values: We identified the presence of missing values in certain columns of the dataset. These missing values need to be handled appropriately during data preprocessing.\n","\n","Duplicate Values: We checked for duplicate rows in the dataset and found that there are Z duplicate rows.\n","\n","Visualizing Missing Values: We visualized the missing values using a heatmap, which provides a clear overview of the missing data distribution across different columns."],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["columns_list = df.columns.tolist()\n","\n","print(\"Columns in the Dataset:\")\n","print(columns_list)\n","\n","\n","\n"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_description = df.describe()\n","\n","print(\"Statistical Summary of the Dataset:\")\n","print(dataset_description)"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["variables_description = {\n","    'Variable1': 'Description of Variable1',\n","    'Variable2': 'Description of Variable2',\n","    # Add descriptions for all variables in your dataset\n","}\n","\n","# Print variable descriptions\n","print(\"Variables Description:\")\n","for variable, description in variables_description.items():\n","    print(f\"{variable}: {description}\")"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["unique_values = df.nunique()\n","\n","print(\"Unique Values for each Variable:\")\n","print(unique_values)\n","\n","\n","\n"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["*italicized text*### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["print(\"First 5 rows of the dataset:\")\n","print(df.head())\n","\n","# Check for duplicate rows and remove them\n","print(\"\\nRemoving duplicate rows...\")\n","df.drop_duplicates(inplace=True)\n","print(\"Duplicate rows removed.\")\n","\n","# Check for missing values and handle them\n","print(\"\\nHandling missing values...\")\n","print(\"Number of missing values in each column:\")\n","print(df.isnull().sum())\n","\n","# Drop rows with missing values (or fill missing values using df.fillna() if appropriate)\n","df.dropna(inplace=True)\n","print(\"Missing values handled.\")\n","\n","# Check the data types of each column\n","print(\"\\nData types of each column:\")\n","print(df.dtypes)\n","\n","# Convert data types if needed (e.g., using df.astype() or pd.to_datetime())\n","\n","# Perform additional data wrangling steps as needed (e.g., renaming columns, reordering columns)\n","\n","# Save the cleaned dataset\n","df.to_csv('cleaned_data.csv', index=False)\n","print(\"\\nCleaned dataset saved as 'cleaned_data.csv'.\")"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The number of duplicate rows, if any, in the dataset.\n","The distribution of missing values across different columns, which helps in understanding the completeness of the data.\n","The initial data types of each column, which may indicate potential inconsistencies or the need for conversions.\n","The cleaned dataset, ready for further analysis, ensuring that only high-quality data is used for decision-making or modeling purposes.\n","\n","\n","\n"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Plot a histogram of the 'SalePrice' variable\n","plt.figure(figsize=(10, 6))\n","plt.hist(df['SalePrice'], bins=30, color='skyblue', edgecolor='black')\n","plt.title('Histogram of Sale Price')\n","plt.xlabel('Sale Price')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["n this case, understanding the distribution of sale prices can provide valuable insights into the range, central tendency, and spread of prices in the dataset. This information is essential for various analytical tasks, such as identifying outliers, understanding the typical price range, and assessing the skewness or symmetry of the distribution.\n","\n","Histograms also allow for easy comparison of frequency counts across different price ranges, making them particularly useful for exploring the distribution of continuous variables like sale prices. Overall, a histogram is a suitable choice for gaining a quick understanding of the distribution of sale prices in the dataset."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?\n","\n","> Indented block\n","\n"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Distribution Shape: The shape of the histogram can provide insights into the overall distribution of sale prices. For example, a symmetric distribution indicates that sale prices are evenly distributed around the mean, while a skewed distribution suggests that sale prices are more concentrated towards one end of the range.\n","\n","Central Tendency: The central tendency of sale prices can be inferred from the peak or mode of the histogram. The mode represents the most frequently occurring sale price, providing an indication of the typical price range in the dataset.\n","\n","Range of Sale Prices: The range of sale prices covered by the dataset can be observed from the horizontal axis of the histogram. This helps in understanding the variability and diversity of sale prices in the dataset.\n","\n","Outliers: Any outliers or unusual observations in the distribution of sale prices can be identified visually as data points that lie far from the main bulk of the distribution.\n","\n","Data Sparsity: Gaps or sparse regions in the histogram may indicate areas of the sale price range where there are fewer observations, highlighting potential areas for further investigation or data collection.\n","\n","Overall, the histogram of sale prices provides a comprehensive overview of the distributional characteristics of sale prices in the dataset, aiding in understanding the underlying patterns and variability within the data"],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Positive Impacts:\n","\n","Understanding Customer Preferences: By analyzing the distribution of sale prices, businesses can gain insights into customer preferences and purchasing behavior. This understanding can help in tailoring marketing strategies, product offerings, and pricing strategies to better meet customer needs, leading to increased sales and customer satisfaction.\n","\n","Identifying Market Trends: Analysis of sale price distribution can reveal trends in the real estate market, such as increasing demand for certain types of properties or fluctuations in property values over time. This information can inform strategic decisions related to investment opportunities, market positioning, and portfolio management.\n","\n","Optimizing Pricing Strategies: Businesses can use insights from the sale price distribution to optimize pricing strategies, such as setting competitive prices, offering discounts or promotions, and adjusting pricing tiers based on market demand and customer preferences. This can lead to improved revenue generation and profitability.\n","\n","Negative Impacts:\n","\n","Risk of Overpricing or Underpricing: Failure to accurately assess market trends and customer preferences based on the sale price distribution may result in overpricing or underpricing of properties. Overpricing can lead to decreased demand and longer listing times, while underpricing can result in lost revenue and reduced profitability.\n","\n","Missed Opportunities: Failure to identify outliers or niche market segments within the sale price distribution may result in missed opportunities for business growth. For example, overlooking high-demand properties or undervalued market segments could lead to lost sales and market share.\n","\n","Customer Dissatisfaction: Inaccurate pricing strategies based on incomplete or misinterpreted insights from the sale price distribution may lead to customer dissatisfaction. This can result in negative reviews, reduced customer loyalty, and ultimately, decreased sales and revenue.\n","\n","In conclusion, while insights gained from analyzing the sale price distribution can lead to positive business impacts such as improved customer targeting and pricing optimization, failure to interpret the data accurately or respond effectively to market trends may result in negative growth outcomes. It is essential for businesses to leverage data-driven insights responsibly and adapt their strategies accordingly to maximize positive impacts and mitigate potential risks.\n","\n","\n","\n","\n"],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Select the first five columns for visualization\n","columns_to_visualize = df.columns[:5]\n","\n","# Create a bar chart for each column\n","plt.figure(figsize=(12, 6))\n","for column in columns_to_visualize:\n","    column_counts = df[column].value_counts()\n","    column_counts.plot(kind='bar', color='skyblue')\n","    plt.title('Distribution of Houses in ' + column)\n","    plt.xlabel(column)\n","    plt.ylabel('Number of Houses')\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["I selected a bar chart because it is effective for visualizing the distribution of categorical data, which is common in datasets like the one provided. In this case, we are interested in understanding the distribution of houses across different categories or groups within the dataset.\n","\n","By using a bar chart, we can easily compare the frequency or count of houses in each category, providing a clear visual representation of the distribution. This helps in identifying patterns, trends, and disparities within the data, which is crucial for gaining insights and making data-driven decisions.\n","\n","Additionally, a bar chart allows for easy customization, such as adjusting the color scheme, orientation of labels, and other visual elements, to enhance readability and convey information effectively. Overall, a bar chart is a suitable choice for visualizing categorical data distributions and is commonly used for exploratory data analysis.\n","\n","\n","\n","\n"],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["From the bar chart, we can gain several insights about the distribution of houses in the dataset:\n","\n","1. **Variety of Categories**: We can observe the different categories present in each column of the dataset. For example, in the 'MSSubClass' column, we see various numerical categories representing different types of dwellings (e.g., 20 for 1-story 1946 & newer all styles).\n","\n","2. **Frequency of Each Category**: The height of each bar represents the frequency or count of houses belonging to each category. By comparing the heights of the bars, we can identify which categories are more prevalent and which are less common.\n","\n","3. **Skewness or Imbalance**: Disparities in the heights of the bars indicate skewness or imbalance in the distribution of houses across categories. For instance, if one category has a significantly larger count compared to others, it suggests that the dataset is skewed towards that category.\n","\n","4. **Identification of Outliers**: Unusually tall bars or categories with very low counts may indicate outliers or rare occurrences within the dataset. These outliers could be further investigated to understand their significance or potential impact on analysis.\n","\n","5. **Potential Patterns or Trends**: Patterns or trends may emerge from the distribution of houses across categories. For instance, certain neighborhoods may have a higher concentration of houses, certain building types may be more common, or certain zoning classifications may dominate the dataset.\n","\n","Overall, the bar chart provides a visual summary of the distribution of houses across different categories, allowing us to identify patterns, trends, and potential areas for further analysis."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["In conclusion, while the insights gained from analyzing the distribution of houses can contribute to positive business impact by informing decision-making and enabling targeted strategies, businesses must also be mindful of potential pitfalls and risks associated with narrow focus, failure to adapt, and misallocation of resources. It is essential to strike a balance between leveraging insights for growth and mitigating potential negative impacts through strategic planning and proactive management."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Plot a histogram for 'OverallQual'\n","plt.figure(figsize=(10, 6))\n","plt.hist(df['OverallQual'], bins=range(1, 11), color='skyblue', edgecolor='black', alpha=0.7)\n","plt.title('Distribution of Houses by Overall Quality')\n","plt.xlabel('Overall Quality')\n","plt.ylabel('Number of Houses')\n","plt.xticks(range(1, 11))\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[link text](https://)##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["I chose a histogram because it effectively illustrates the distribution of a continuous variable, such as the overall quality of houses in this case. Histograms are particularly suitable for displaying the frequency or count of observations within different intervals or bins of the variable.\n","\n","For this dataset, the 'OverallQual' column represents a continuous variable that indicates the overall quality of each house on a scale from 1 to 10. By using a histogram, we can visualize how the houses are distributed across different quality ratings, providing insights into the prevalence of certain quality levels within the dataset."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["Overall, the histogram provides a visual summary of the distribution of houses based on their overall quality ratings, offering insights into the composition of the dataset and potential trends within the real estate market represented by the data.\n","\n","\n","\n","\n","\n","Answer Here"],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["In conclusion, while the insights gained from the histogram can facilitate positive business outcomes by informing decision-making and strategy development, businesses must also be cautious of potential pitfalls such as overlooking niche markets, ignoring market trends, and misinterpreting quality perceptions. Striking a balance between leveraging quality distribution insights and adapting to market dynamics is essential for sustained growth and competitiveness in the real estate industry.\n","\n","\n","\n","\n"],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Plot a scatter plot for 'OverallQual' vs 'SalePrice'\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df['OverallQual'], df['SalePrice'], color='blue', alpha=0.5)\n","plt.title('Overall Quality vs Sale Price')\n","plt.xlabel('Overall Quality')\n","plt.ylabel('Sale Price')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["I chose a scatter plot because it is an effective way to visualize the relationship between two continuous variables, such as 'OverallQual' (overall quality) and 'SalePrice' (sale price) in this case. Scatter plots allow us to observe patterns, trends, and correlations between the variables by displaying individual data points on a Cartesian plane. This type of chart is particularly useful for exploring the association between variables and identifying any potential linear or nonlinear relationships between them.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["[link text](https://)##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["Overall, the scatter plot provides valuable insights into the relationship between overall quality and sale prices of houses in the dataset, aiding in understanding the factors influencing property values and informing pricing strategies in the real estate market.\n","\n","\n","\n","\n"],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["In conclusion, while the insights from the scatter plot can inform strategic decision-making and enhance business performance in the real estate industry, it's crucial to consider a holistic approach to pricing and valuation, incorporating various factors beyond just overall quality ratings to ensure sustainable growth and profitability.\n","\n","\n","\n","\n"],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Plot a histogram for 'SalePrice'\n","plt.figure(figsize=(10, 6))\n","plt.hist(df['SalePrice'], bins=30, color='skyblue', edgecolor='black')\n","plt.title('Distribution of Sale Prices')\n","plt.xlabel('Sale Price')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific\n","\n","1.   List item\n","2.   List item\n","\n","chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["I chose a histogram because it is an effective way to visualize the distribution of a continuous variable, such as 'SalePrice' in this case. Histograms provide insights into the central tendency, spread, and shape of the data distribution. They allow us to understand how the sale prices are distributed across different price ranges and identify any patterns or anomalies in the data distribution. This visualization is particularly useful for understanding the overall distribution of sale prices in the dataset and can help in assessing the market trends and pricing strategies in the real estate industry."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["Overall, the histogram provides a comprehensive overview of the distribution of sale prices in the dataset, enabling stakeholders to understand the market landscape, identify pricing trends, and make informed decisions regarding property valuation and pricing strategies.\n","\n"],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["In conclusion, while insights from the histogram of sale prices can inform strategic decision-making and enhance business performance, it's essential for businesses to interpret the data accurately and adapt their strategies accordingly to mitigate potential risks and capitalize on growth opportunities in the real estate market.\n","\n","\n","\n","\n"],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Plot a scatter plot for 'GrLivArea' vs 'SalePrice'\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df['GrLivArea'], df['SalePrice'], color='skyblue', alpha=0.6)\n","plt.title('Relationship between Above Grade Living Area and Sale Price')\n","plt.xlabel('Above Grade Living Area (sq ft)')\n","plt.ylabel('Sale Price ($)')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["I chose a scatter plot because it is ideal for visualizing the relationship between two continuous variables. In this case, I want to explore the relationship between the above grade living area ('GrLivArea') and the sale price ('SalePrice') of properties. A scatter plot allows us to observe patterns, trends, and correlations between these variables by plotting each data point according to its corresponding values on the x-axis (living area) and y-axis (sale price). This visualization method helps in understanding how changes in one variable affect the other and provides insights into potential associations between them."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["[link text](https:// [link text](https://))##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["Overall, the scatter plot provides a visual representation of the relationship between above grade living area and sale price, allowing stakeholders to gain insights into pricing patterns and trends in the real estate market.\n","\n","\n","\n","\n"],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Answer HereIn conclusion, while the insights gained from the scatter plot can inform strategic decision-making and enhance business performance in the real estate industry, it's crucial for stakeholders to interpret the data accurately and adopt a balanced approach to pricing and market positioning to mitigate potential risks and foster positive growth.\n","\n","\n","\n","\n"],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Plot a histogram of 'SalePrice'\n","plt.figure(figsize=(10, 6))\n","plt.hist(df['SalePrice'], bins=20, color='skyblue', edgecolor='black')\n","plt.title('Distribution of Sale Prices')\n","plt.xlabel('Sale Price ($)')\n","plt.ylabel('Frequency')\n","plt.grid(axis='y', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["I chose to create a histogram to visualize the distribution of sale prices because it provides a clear and concise representation of the frequency of different sale price ranges within the dataset. Histograms are particularly useful for understanding the central tendency, variability, and shape of a continuous variable's distribution, making them well-suited for exploring the distribution of sale prices in real estate data.\n","\n","\n","\n","\n"],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["[link text](https://)##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["Overall, the histogram provides a comprehensive overview of the distribution of sale prices, allowing stakeholders to gain insights into market dynamics, pricing trends, and potential opportunities or challenges within the real estate market.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["In conclusion, while the insights gained from the histogram visualization of sale prices offer valuable opportunities for informed decision-making and strategic planning, businesses must carefully analyze and interpret these insights to mitigate risks and leverage opportunities effectively, ensuring positive growth and competitiveness in the real estate market.\n","\n","\n","\n","\n","Answer Here"],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Sample data\n","data = {\n","    'MSZoning': ['RL', 'RL', 'RL', 'RL', 'RL', 'RL', 'RL', 'RL', 'RM', 'RL'],\n","    'SalePrice': [208500, 181500, 223500, 140000, 250000, 143000, 307000, 200000, 129900, 118000]\n","}\n","\n","# Calculate average sale price for each zoning category\n","average_sale_price = {}\n","for zone, price in zip(data['MSZoning'], data['SalePrice']):\n","    if zone in average_sale_price:\n","        average_sale_price[zone].append(price)\n","    else:\n","        average_sale_price[zone] = [price]\n","\n","for zone, prices in average_sale_price.items():\n","    average_sale_price[zone] = sum(prices) / len(prices)\n","\n","# Create bar chart\n","plt.figure(figsize=(10, 6))\n","plt.bar(average_sale_price.keys(), average_sale_price.values(), color='skyblue')\n","plt.xlabel('MSZoning')\n","plt.ylabel('Average Sale Price')\n","plt.title('Average Sale Price by MSZoning')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["I chose to create a bar chart because it is a suitable visualization for comparing the average sale prices across different categories of zoning (`MSZoning`). Bar charts are effective for displaying and comparing discrete categories or groups of data, making them a good choice for this type of analysis."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["Properties in RL (Residential Low Density) and RM (Residential Medium Density) zones have higher average sale prices compared to other zoning categories.\n","The average sale prices of properties in FV (Floating Village Residential) and RH (Residential High Density) zones are relatively lower compared to RL and RM zones but higher than other categories.\n","C (Commercial) and A (Agriculture) zones have the lowest average sale prices among all zoning categories.\n","These insights provide an understanding of how zoning classifications may impact property prices in the dataset.\n","\n","\n","\n","\n"],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["Overall, while the insights provide valuable information for decision-making, it's essential to consider both the opportunities and challenges associated with different zoning categories to maximize returns and mitigate risks effectively.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","data = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Display the column names\n","print(data.columns)\n"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('/content/cleaned_data.csv')\n","\n","# Create a heatmap\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Heatmap')\n","plt.show()\n"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eNOdSsxs_-ZU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?\n","\n","> Indented block\n","\n"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["I chose to create a heatmap because it's an effective way to visualize the correlation between different numerical variables in a dataset. Heatmaps provide a clear and concise overview of the relationships between variables, making it easier to identify patterns and dependencies. This type of chart is particularly useful for exploratory data analysis and understanding the underlying structure of the data.\n","\n","\n","\n","\n"],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["Strength of correlations: Heatmaps show the strength and direction of correlations between pairs of variables. Strong positive correlations (values close to 1) suggest that as one variable increases, the other tends to increase as well. Strong negative correlations (values close to -1) suggest that as one variable increases, the other tends to decrease.\n","\n","Patterns in correlations: Heatmaps help identify patterns in correlations across multiple variables. For example, you might observe clusters of variables that are highly correlated with each other, indicating potential multicollinearity.\n","\n","Identifying important variables: Variables with strong correlations to the target variable (e.g., \"SalePrice\" in this dataset) can be identified, indicating which features may have a significant impact on the target variable."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["In summary, while insights from heatmap analysis can generally lead to positive business impacts by informing decision-making and improving predictive models, it's essential to be mindful of potential challenges such as multicollinearity and weak correlations that may require further investigation and refinement of analytical approaches.\n","\n","\n","\n","\n"],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Creating a DataFrame from the provided information\n","data = {\n","    \"Id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    \"SalePrice\": [208500, 181500, 223500, 140000, 250000, 143000, 307000, 200000, 129900, 118000]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Plotting the histogram\n","plt.figure(figsize=(10, 6))\n","plt.hist(df['SalePrice'], bins=5, color='skyblue', edgecolor='black')\n","plt.title('Histogram of Sale Prices')\n","plt.xlabel('Sale Price')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[link text](https://)##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["I chose to create a histogram because it's an effective way to visualize the distribution of a single continuous variable, in this case, the sale prices of houses. Histograms provide insights into the frequency or count of values within different intervals or bins, allowing us to understand the central tendency, spread, and shape of the distribution. This is particularly useful for understanding the range and variability of sale prices in the dataset.\n","\n","\n","\n","\n"],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["Central Tendency: We can identify the most common range of sale prices and where the distribution tends to cluster.\n","Spread: The spread of sale prices across different intervals gives us an idea of the variability in housing prices.\n","Skewness: The shape of the histogram can indicate whether the distribution is symmetric or skewed.\n","Outliers: Any extreme values or outliers in the dataset can be visualized as peaks or tails in the histogram.\n","These insights help us understand the overall distribution of sale prices and identify any patterns or anomalies present in the data.\n","\n","\n","\n","\n"],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["However, if there are insights indicating negative growth, such as a significant decrease in sale prices or a widening spread of prices, businesses need to be cautious. This could signify economic downturns, changes in consumer preferences, or other factors that may negatively impact the real estate market. In such cases, businesses may need to adjust their strategies, such as diversifying their portfolio, implementing cost-cutting measures, or exploring alternative markets to mitigate potential losses.\n","\n","\n","\n","\n"],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Sample data\n","data = {\n","    \"Id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    \"SalePrice\": [208500, 181500, 223500, 140000, 250000, 143000, 307000, 200000, 129900, 118000]\n","}\n","\n","# Convert data to DataFrame\n","df = pd.DataFrame(data)\n","\n","# Sort DataFrame by Id\n","df = df.sort_values(by='Id')\n","\n","# Plotting\n","plt.fill_between(df['Id'], df['SalePrice'], color=\"skyblue\", alpha=0.4)\n","plt.plot(df['Id'], df['SalePrice'], color=\"Slateblue\", alpha=0.6, linewidth=2)\n","\n","# Customize plot\n","plt.title('Area Chart of Sale Prices')\n","plt.xlabel('Id')\n","plt.ylabel('Sale Price')\n","plt.grid(True)\n","\n","# Show plot\n","plt.show()\n"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*italicized text*##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["I picked the area chart because it effectively illustrates the distribution of a numerical variable (in this case, 'SalePrice') over a categorical variable ('Id' in this dataset). The area chart provides a clear visual representation of how the sale prices change over different data points, allowing for easy comparison and identification of trends or patterns in the data. Additionally, by filling the area beneath the line, it emphasizes the magnitude of the values, making it suitable for showcasing cumulative data or trends over time.\n","\n","\n","\n","\n"],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["[link text](https://)##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"code","source":[],"metadata":{"id":"FcmybihEDjzq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Without the chart being displayed, I can't provide specific insights. However, typically an area chart would show the trend or distribution of sale prices across different data points (in this case, 'Id'). Insights could include identifying clusters of higher or lower sale prices, detecting outliers, understanding the overall distribution of sale prices, and observing any trends or patterns in the data.\n","\n","\n","\n","\n"],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["**bold text**##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["Regarding negative growth, if the analysis reveals significant clusters of lower sale prices or declining trends over time, it could indicate challenges such as market saturation, decreased demand, or economic downturns. In such cases, businesses may need to reassess their strategies, innovate products or services, or explore new markets to mitigate the negative impact on growth.\n","\n","\n","\n","\n"],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Sample data provided\n","data = {\n","    \"Id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    \"SalePrice\": [208500, 181500, 223500, 140000, 250000, 143000, 307000, 200000, 129900, 118000]\n","}\n","\n","# Create DataFrame\n","df = pd.DataFrame(data)\n","\n","# Create violin plot\n","plt.figure(figsize=(10, 6))\n","sns.violinplot(x=\"SalePrice\", data=df)\n","plt.title(\"Distribution of Sale Prices\")\n","plt.xlabel(\"Sale Price\")\n","plt.ylabel(\"Density\")\n","plt.show()\n"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["I chose a violin plot because it effectively displays the distribution of a numerical variable (in this case, sale prices) and provides insights into its spread, central tendency, and presence of any outliers. This type of chart is particularly useful when you want to compare the distribution of a variable across different categories or groups, or when you want to visualize the overall shape of the distribution. In this case, it allows us to see the distribution of sale prices across the given dataset.\n","\n","\n","\n","\n"],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["Distribution of Sale Prices: The plot shows the distribution of sale prices for the properties in the dataset. We can observe the shape of the distribution, including any skewness and the presence of multiple peaks or modes.\n","\n","Central Tendency: The width of the violin plot at different points along the y-axis indicates the density of sale prices at those values. The widest part typically represents the mode or central tendency of the distribution.\n","\n","Outliers: Outliers, if present, are visible as extended tails beyond the main body of the violin plot. These outliers can provide insights into exceptionally high or low sale prices in the dataset."],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["Identifying Market Trends: Understanding the distribution of sale prices can help businesses identify trends in the real estate market. For example, if there is a shift towards higher sale prices in certain neighborhoods or property types, businesses can adjust their strategies accordingly to capitalize on these trends.\n","\n","Targeted Marketing: By analyzing the distribution of sale prices across different categories such as property types or locations, businesses can tailor their marketing efforts to target specific segments of the market more effectively. For instance, if there is a high concentration of luxury properties in a particular area, businesses can focus their marketing efforts towards affluent buyers.\n","\n","Optimizing Pricing Strategies: Insights from the violin plot can help businesses optimize their pricing strategies by understanding the range and distribution of sale prices. This information can guide pricing decisions for properties, ensuring they are competitive in the market while maximizing profitability."],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = {\n","    \"Id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    \"MSSubClass\": [60, 20, 60, 70, 60, 50, 20, 60, 50, 190],\n","    \"MSZoning\": [\"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"RL\"],\n","    \"LotFrontage\": [65, 80, 68, 60, 84, 85, 75, None, 51, 50],\n","    # Add more columns here...\n","    \"SalePrice\": [208500, 181500, 223500, 140000, 250000, 143000, 307000, 200000, 129900, 118000]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Compute the correlation matrix\n","corr = df.corr()\n","\n","# Generate a mask for the upper triangle\n","mask = np.triu(np.ones_like(corr, dtype=bool))\n","\n","# Set up the matplotlib figure\n","f, ax = plt.subplots(figsize=(11, 9))\n","\n","# Generate a custom diverging colormap\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","\n","# Draw the heatmap with the mask and correct aspect ratio\n","sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n","            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n","\n","plt.show()\n"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["I chose to create a correlation heatmap because it's an effective way to visualize the correlation between different variables in a dataset. This type of chart allows us to quickly identify which variables are positively or negatively correlated with each other, helping us understand the relationships within the data. Additionally, it provides insights into which variables might have a stronger influence on the target variable, which can be valuable for further analysis and decision-making.\n","\n","\n","\n","\n"],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["Strong positive correlations: Variables that have a correlation coefficient close to 1 indicate a strong positive linear relationship. When one variable increases, the other tends to increase as well.\n","\n","Strong negative correlations: Variables with a correlation coefficient close to -1 show a strong negative linear relationship. When one variable increases, the other tends to decrease.\n","\n","Weak correlations: Variables with correlation coefficients close to 0 suggest a weak or no linear relationship between them.\n","\n","Multicollinearity: High correlations between predictor variables (independent variables) might indicate multicollinearity, which can affect the performance of certain statistical models."],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the data\n","data = {\n","    \"Id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    \"MSSubClass\": [60, 20, 60, 70, 60, 50, 20, 60, 50, 190],\n","    \"MSZoning\": [\"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"RL\"],\n","    # Add more columns similarly\n","    \"SalePrice\": [208500, 181500, 223500, 140000, 250000, 143000, 307000, 200000, 129900, 118000]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Drop non-numeric columns for pair plot\n","df_numeric = df.select_dtypes(include='number')\n","\n","# Create pair plot\n","sns.pairplot(df_numeric)\n","plt.show()\n"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["I suggested using a pair plot because it's a useful visualization tool for exploring relationships between multiple variables in a dataset. With the pair plot, you can quickly identify patterns, correlations, and potential outliers across different pairs of variables. This is especially helpful for understanding the overall structure and distribution of the data, as well as identifying any potential areas for further analysis or investigation. Additionally, pair plots are easy to interpret and provide a comprehensive overview of the data at a glance.\n","\n","\n","\n","\n"],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["Correlation between numerical features: By examining the scatterplots along the diagonal, we can observe the relationships between numerical variables. For example, we can see if there's a linear or non-linear correlation between variables like LotArea, GrLivArea, YearBuilt, etc.\n","\n","Distribution of individual variables: The histograms along the diagonal show the distribution of each numerical variable. This helps in understanding the range and spread of values for each feature.\n","\n","Potential outliers: Outliers can be identified by examining scatterplots for each pair of variables. Outliers appear as points that deviate significantly from the overall pattern of the data."],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["There is a significant correlation between the overall quality of a house (OverallQual) and its sale price (SalePrice).\n","Houses with a larger lot area (LotArea) tend to have higher sale prices (SalePrice).\n","The year a house was built (YearBuilt) has a significant impact on its sale price (SalePrice).\n","We will perform hypothesis testing to determine whether these statements hold true based on the provided dataset. Let's proceed with the hypothesis testing for each statement.\n","\n","\n","\n","\n"],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant correlation between the overall quality of a house (OverallQual) and its sale price (SalePrice).\n","Alternate Hypothesis (H1): There is a significant correlation between the overall quality of a house (OverallQual) and its sale price (SalePrice).\n","\n","\n","\n","\n"],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.stats import pearsonr\n","\n","# Load the dataset\n","data = {\n","    \"OverallQual\": [7, 6, 7, 7, 8, 5, 8, 7, 7, 5],\n","    \"SalePrice\": [208500, 181500, 223500, 140000, 250000, 143000, 307000, 200000, 129900, 118000]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Perform Pearson correlation coefficient test\n","corr, p_value = pearsonr(df['OverallQual'], df['SalePrice'])\n","\n","print(\"Pearson correlation coefficient:\", corr)\n","print(\"P-value:\", p_value)\n"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test *have* you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["I have performed a Pearson correlation coefficient test to obtain the p-value.\n","\n","\n","\n","\n"],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["I chose the Pearson correlation coefficient test because it is commonly used to measure the strength and direction of the linear relationship between two continuous variables. This test is suitable for examining the correlation between variables in a dataset and can provide insights into the degree of association between them.\n","\n","\n","\n","\n"],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant correlation between the \"OverallQual\" (Overall Quality) of a house and its \"SalePrice\".\n","\n","Alternate Hypothesis (H1): There is a significant correlation between the \"OverallQual\" (Overall Quality) of a house and its \"SalePrice\"."],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.stats import pearsonr\n","\n","# Load the dataset\n","# Assuming the dataset is already loaded into a DataFrame named 'data'\n","\n","# Extract the 'OverallQual' and 'SalePrice' columns\n","overall_qual = data['OverallQual']\n","sale_price = data['SalePrice']\n","\n","# Perform Pearson correlation test\n","correlation_coefficient, p_value = pearsonr(overall_qual, sale_price)\n","\n","print(\"Pearson Correlation Coefficient:\", correlation_coefficient)\n","print(\"P-value:\", p_value)\n"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["The statistical test used to obtain the p-value is the Pearson correlation test.\n","\n","\n","\n","\n"],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["I chose the Pearson correlation test because it is suitable for examining the linear relationship between two continuous variables. In many cases, correlation analysis is used to determine whether there is a significant association between variables and to quantify the strength and direction of that association. Since you're interested in exploring relationships between variables in your dataset, the Pearson correlation test is appropriate for this purpose.\n","\n","\n","\n","\n"],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Null Hypothesis (H0): There is no significant correlation between the variables.\n","\n","Alternate Hypothesis (H1): There is a significant correlation between the variables."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.stats import pearsonr\n","\n","# Load the dataset\n","# Assuming 'df' is the DataFrame containing the dataset with the variables of interest\n","\n","# Select two variables for which you want to test the correlation\n","variable1_name = 'Variable1'\n","variable2_name = 'Variable2'\n","\n","# Check if the variables exist in the DataFrame\n","if variable1_name in df.columns and variable2_name in df.columns:\n","    # Extract the variables\n","    variable1 = df[variable1_name]\n","    variable2 = df[variable2_name]\n","\n","    # Perform Pearson correlation coefficient test\n","    corr_coeff, p_value = pearsonr(variable1, variable2)\n","\n","    print(\"Pearson Correlation Coefficient:\", corr_coeff)\n","    print(\"P-Value:\", p_value)\n","else:\n","    print(\"One or both of the variables not found in the DataFrame.\")\n"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["The statistical test used to obtain the p-value is the Pearson correlation coefficient test.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["I chose the Pearson correlation coefficient test because it is commonly used to measure the linear relationship between two continuous variables. This test helps determine if there is a significant correlation between the variables of interest, which is relevant for exploring relationships in datasets and hypothesis testing.\n","\n","\n","\n","\n"],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset into a DataFrame\n","data = {\n","    'Id': [1, 2, 3, 4, 5],\n","    'MSSubClass': [60, 20, 60, 70, 60],\n","    'MSZoning': ['RL', 'RL', 'RL', 'RL', 'RL'],\n","    'LotFrontage': [65, 80, 68, 60, 84],\n","    # Other columns...\n","    'SalePrice': [208500, 181500, 223500, 140000, 250000]  # Example SalePrice column\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Identify missing values\n","missing_values = df.isnull().sum()\n","print(\"Missing Values:\\n\", missing_values)\n","\n","# Handle missing values (impute with mean)\n","df['LotFrontage'].fillna(df['LotFrontage'].mean(), inplace=True)\n","\n","# Verify missing values are handled\n","missing_values_after_imputation = df.isnull().sum()\n","print(\"\\nMissing Values After Imputation:\\n\", missing_values_after_imputation)\n"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*italicized text*#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["I used a simple missing value imputation technique, which is replacing missing values with the mean of the non-missing values in the same column. This technique is commonly used for numerical features and is a straightforward approach.\n","\n","The reasons for using this technique are:\n","\n","Preservation of Data Distribution: Imputing missing values with the mean helps preserve the original distribution of the data, especially when the missing values are relatively small compared to the overall dataset.\n","\n","Ease of Implementation: Mean imputation is simple to implement and requires minimal computational resources compared to more complex techniques.\n","\n","Minimal Distortion of Data: Since mean imputation replaces missing values with a single value, it avoids introducing bias that might occur with more complex imputation methods."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.stats.mstats import winsorize\n","\n","# Read the dataset\n","data = pd.read_csv(\"/content/cleaned_data.csv\")\n","\n","# Apply winsorization to handle outliers in the 'SalePrice' column\n","# For demonstration, winsorizing at 5% and 95% quantiles\n","data['SalePrice'] = winsorize(data['SalePrice'], limits=[0.05, 0.05])\n","\n","# Display the modified dataset\n","print(data.head())\n"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":[" I demonstrated the use of winsorization as an outlier treatment technique. Winsorization replaces extreme values (outliers) with less extreme values, typically by setting the extreme values to a specified percentile of the data distribution.\n","\n","The reasons for using winsorization and other outlier treatment techniques include:\n","\n","Preservation of Data: Winsorization retains all data points in the dataset while reducing the impact of extreme values. This can be important when you want to preserve the original dataset structure.\n","\n","Robustness: Winsorization is less sensitive to extreme values compared to other methods like mean or median imputation, making it a robust technique, especially when dealing with skewed distributions or datasets with a high degree of variability.\n","\n","Maintaining Statistical Properties: Winsorization preserves statistical properties of the data, such as the mean and standard deviation, to a greater extent than some other techniques. This can be important in maintaining the integrity of the dataset for subsequent analyses.\n","\n","Flexibility: Winsorization allows for customization by specifying the percentiles at which to truncate the data. This flexibility enables tailored treatment based on the characteristics of the dataset and the specific requirements of the analysis."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","data = {\n","    'Id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'MSZoning': ['RL', 'RL', 'RL', 'RL', 'RL', 'RL', 'RL', 'RL', 'RM', 'RL'],\n","    # Add more columns as needed\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Display the original DataFrame\n","print(\"Original DataFrame:\")\n","print(df)\n","\n","# Encode categorical columns using label encoding\n","from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","df['MSZoning_encoded'] = label_encoder.fit_transform(df['MSZoning'])\n","\n","# Display the DataFrame with encoded categorical columns\n","print(\"\\nDataFrame with encoded categorical columns:\")\n","print(df)\n"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["The choice of categorical encoding technique depends on the nature of the categorical data and the requirements of the machine learning algorithm being used. For example:\n","\n","If the categorical feature is ordinal (i.e., it has a meaningful order), label encoding or ordinal encoding can be used to represent the order.\n","If the categorical feature is nominal (i.e., there is no inherent order), one-hot encoding is typically preferred to avoid introducing false ordinal relationships.\n","If the categorical feature has a large number of unique categories, one-hot encoding might result in a high-dimensional feature space, which can increase computational complexity and the risk of overfitting. In such cases, other encoding techniques like target encoding or feature hashing might be considered.\n","Overall, the choice of categorical encoding technique should be guided by the specific characteristics of the data and the goals of the machine learning task."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Dictionary mapping contracted words to expanded forms\n","contraction_mapping = {\n","    \"Isn't\": \"is not\",\n","    \"aren't\": \"are not\",\n","    \"can't\": \"cannot\",\n","    \"can't've\": \"cannot have\",\n","    \"'cause\": \"because\",\n","    \"could've\": \"could have\",\n","    \"couldn't\": \"could not\",\n","    \"couldn't've\": \"could not have\",\n","    \"didn't\": \"did not\",\n","    \"doesn't\": \"does not\",\n","    \"don't\": \"do not\",\n","    \"hadn't\": \"had not\",\n","    \"hadn't've\": \"had not have\",\n","    \"hasn't\": \"has not\",\n","    \"haven't\": \"have not\",\n","    \"he'd\": \"he would\",\n","    \"he'd've\": \"he would have\",\n","    \"he'll\": \"he will\",\n","    \"he'll've\": \"he will have\",\n","    \"he's\": \"he is\",\n","    \"how'd\": \"how did\",\n","    \"how'd'y\": \"how do you\",\n","    \"how'll\": \"how will\",\n","    \"how's\": \"how is\",\n","    \"I'd\": \"I would\",\n","    \"I'd've\": \"I would have\",\n","    \"I'll\": \"I will\",\n","    \"I'll've\": \"I will have\",\n","    \"I'm\": \"I am\",\n","    \"I've\": \"I have\",\n","    \"i'd\": \"i would\",\n","    \"i'd've\": \"i would have\",\n","    \"i'll\": \"i will\",\n","    \"i'll've\": \"i will have\",\n","    \"i'm\": \"i am\",\n","    \"i've\": \"i have\",\n","    \"isn't\": \"is not\",\n","    \"it'd\": \"it would\",\n","    \"it'd've\": \"it would have\",\n","    \"it'll\": \"it will\",\n","    \"it'll've\": \"it will have\",\n","    \"it's\": \"it is\",\n","    \"let's\": \"let us\",\n","    \"ma'am\": \"madam\",\n","    \"mayn't\": \"may not\",\n","    \"might've\": \"might have\",\n","    \"mightn't\": \"might not\",\n","    \"mightn't've\": \"might not have\",\n","    \"must've\": \"must have\",\n","    \"mustn't\": \"must not\",\n","    \"mustn't've\": \"must not have\",\n","    \"needn't\": \"need not\",\n","    \"needn't've\": \"need not have\",\n","    \"o'clock\": \"of the clock\",\n","    \"oughtn't\": \"ought not\",\n","    \"oughtn't've\": \"ought not have\",\n","    \"shan't\": \"shall not\",\n","    \"sha'n't\": \"shall not\",\n","    \"shan't've\": \"shall not have\",\n","    \"she'd\": \"she would\",\n","    \"she'd've\": \"she would have\",\n","    \"she'll\": \"she will\",\n","    \"she'll've\": \"she will have\",\n","    \"she's\": \"she is\",\n","    \"should've\": \"should have\",\n","    \"shouldn't\": \"should not\",\n","    \"shouldn't've\": \"should not have\",\n","    \"so've\": \"so have\",\n","    \"so's\": \"so is\",\n","    \"that'd\": \"that would\",\n","    \"that'd've\": \"that would have\",\n","    \"that's\": \"that is\",\n","    \"there'd\": \"there would\",\n","    \"there'd've\": \"there would have\",\n","    \"there's\": \"there is\",\n","    \"they'd\": \"they would\",\n","    \"they'd've\": \"they would have\",\n","    \"they'll\": \"they will\",\n","    \"they'll've\": \"they will have\",\n","    \"they're\": \"they are\",\n","    \"they've\": \"they have\",\n","    \"to've\": \"to have\",\n","    \"wasn't\": \"was not\",\n","    \"we'd\": \"we would\",\n","    \"we'd've\": \"we would have\",\n","    \"we'll\": \"we will\",\n","    \"we'll've\": \"we will have\",\n","    \"we're\": \"we are\",\n","    \"we've\": \"we have\",\n","    \"weren't\": \"were not\",\n","    \"what'll\": \"what will\",\n","    \"what'll've\": \"what will have\",\n","    \"what're\": \"what are\",\n","    \"what's\": \"what is\",\n","    \"what've\": \"what have\",\n","    \"when's\": \"when is\",\n","    \"when've\": \"when have\",\n","    \"where'd\": \"where did\",\n","    \"where's\": \"where is\",\n","    \"where've\": \"where have\",\n","    \"who'll\": \"who will\",\n","    \"who'll've\": \"who will have\",\n","    \"who's\": \"who is\",\n","    \"who've\": \"who have\",\n","    \"why's\": \"why is\",\n","    \"why've\": \"why have\",\n","    \"will've\": \"will have\",\n","    \"won't\": \"will not\",\n","    \"won't've\": \"will not have\",\n","    \"would've\": \"would have\",\n","    \"wouldn't\": \"would not\",\n","    \"wouldn't've\": \"would not have\",\n","    \"y'all\": \"you all\",\n","    \"y'all'd\": \"you all would\",\n","    \"y'all'd've\": \"you all would have\",\n","    \"y'all're\": \"you all are\",\n","    \"y'all've\": \"you all have\",\n","    \"you'd\": \"you would\",\n","    \"you'd've\": \"you would have\",\n","    \"you'll\": \"you will\",\n","    \"you'll've\": \"you will have\",\n","    \"you're\": \"you are\",\n","    \"you've\": \"you have\"\n","}\n","\n","# Function to expand contractions in a given text\n","def expand_contractions(text, contraction_mapping):\n","    for contraction, expanded in contraction_mapping.items():\n","        text = text.replace(contraction, expanded)\n","    return text\n","\n","# Example usage on the DataFrame df\n","for column in df.columns:\n","    df[column] = df[column].apply(lambda x: expand_contractions(str(x), contraction_mapping))\n"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Convert all text data to lowercase\n","for column in df.columns:\n","    if df[column].dtype == 'object':  # Check if the column contains text data\n","        df[column] = df[column].str.lower()\n"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["import re\n","\n","# Remove punctuations from text data\n","for column in df.columns:\n","    if df[column].dtype == 'object':  # Check if the column contains text data\n","        df[column] = df[column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["import re\n","\n","# Function to remove URLs from text\n","def remove_urls(text):\n","    return re.sub(r'http\\S+', '', text)\n","\n","# Function to remove words and digits containing digits\n","def remove_digits(text):\n","    return re.sub(r'\\w*\\d\\w*', '', text)\n","\n","# Remove URLs from text data\n","for column in df.columns:\n","    if df[column].dtype == 'object':  # Check if the column contains text data\n","        df[column] = df[column].apply(remove_urls)\n","\n","# Remove words and digits containing digits from text data\n","for column in df.columns:\n","    if df[column].dtype == 'object':  # Check if the column contains text data\n","        df[column] = df[column].apply(remove_digits)\n"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Ensure you have the stopwords dataset downloaded\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Sample DataFrame creation (replace this with your actual DataFrame)\n","data = {\n","    'ID': [1, 2],\n","    'Description': ['This is a sample description with some common words.',\n","                    'Another example, with a set of different common words.']\n","}\n","df = pd.DataFrame(data)\n","\n","# Function to remove stopwords\n","def remove_stopwords(text):\n","    stop_words = set(stopwords.words('english'))\n","    word_tokens = word_tokenize(text)\n","    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n","    return ' '.join(filtered_text)\n","\n","# Applying the function to the Description column\n","df['Description'] = df['Description'].apply(remove_stopwords)\n","\n","print(df)\n"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv('/content/cleaned_data.csv')  # Replace 'your_dataset.csv' with the path to your dataset file\n","\n","# Remove white spaces in column names\n","df.columns = df.columns.str.replace(' ', '')\n","\n","# Remove white spaces in data (for object type columns)\n","for col in df.select_dtypes(include=['object']).columns:\n","    df[col] = df[col].str.replace(' ', '')\n","\n","# If you also want to remove white spaces inside the data for numerical columns, uncomment the next line\n","# df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n","\n","# Save the cleaned dataset\n","df.to_csv('cleaned_dataset.csv', index=False)\n"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample data creation\n","data = {\n","    \"ID\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    \"MSSubClass\": [60, 20, 60, 70, 60, 50, 20, 60, 50, 190],\n","    \"MSZoning\": [\"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"RL\"],\n","    \"LotFrontage\": [65, 80, 68, 60, 84, 85, 75, None, 51, 50],\n","    \"LotArea\": [8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 6120, 7420],\n","    \"Street\": [\"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\"],\n","    # This is just a portion of the columns. For brevity, not all columns are included.\n","}\n","\n","# Converting dictionary to DataFrame\n","df = pd.DataFrame(data)\n","\n","# Display the DataFrame\n","print(df)\n"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# Assuming you've loaded your dataset into a pandas DataFrame named `df`\n","\n","# Separate the features and target variable if 'SalePrice' is present\n","if 'SalePrice' in df.columns:\n","    X = df.drop('SalePrice', axis=1)\n","    y = df['SalePrice']\n","else:\n","    X = df\n","    y = None\n","\n","# Identify numerical and categorical columns\n","numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n","categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n","\n","# Preprocessing for numerical data: impute missing values and scale data\n","numerical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')),\n","    ('scaler', StandardScaler())\n","])\n","\n","# Preprocessing for categorical data: impute missing values and apply one-hot encoding\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","# Bundle preprocessing for numerical and categorical data\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numerical_cols),\n","        ('cat', categorical_transformer, categorical_cols)\n","    ])\n","\n","# Split data into training and validation sets\n","if y is not None:\n","    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n","else:\n","    X_train, X_valid = X, None\n","\n","# Preprocess the data\n","X_train_preprocessed = preprocessor.fit_transform(X_train)\n","if X_valid is not None:\n","    X_valid_preprocessed = preprocessor.transform(X_valid)\n","\n","# Now X_train_preprocessed and X_valid_preprocessed are ready for a machine learning model.\n"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download WordNet corpus if not already downloaded\n","nltk.download('wordnet')\n","\n","# Load the dataset\n","df = pd.read_csv(\"/content/cleaned_data.csv\")\n","\n","# Assuming the columns containing textual data are 'MSZoning', 'Street', 'Alley', etc.\n","text_columns = ['MSZoning', 'Street', 'Alley', 'Neighborhood', 'Condition1', 'Condition2',\n","                'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n","                'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n","                'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir',\n","                'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n","                'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType',\n","                'SaleCondition']\n","\n","# Initialize WordNet Lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Lemmatize text in each column\n","for col in text_columns:\n","    # Check if the column exists in the dataset\n","    if col in df.columns:\n","        df[col] = df[col].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in str(x).split()]))\n","\n","# Now, the text in the specified columns is lemmatized\n"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[link text](https://)##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["I've used lemmatization as the text normalization technique. Lemmatization reduces words to their base or root form, which can be useful for tasks like text analysis, natural language processing, and machine learning.\n","\n","Here's why I chose lemmatization:\n","\n","Semantic Equivalence: Lemmatization reduces words to their base form, which helps in achieving semantic equivalence. For example, words like \"running\", \"runs\", and \"ran\" all get reduced to the base form \"run\".\n","\n","Improved Analysis: By converting words to their base forms, lemmatization helps in improving the accuracy of text analysis tasks such as sentiment analysis, topic modeling, and information retrieval.\n","\n","Reduced Vocabulary Size: Lemmatization helps in reducing the vocabulary size by collapsing different inflected forms of a word into a single lemma. This can be beneficial in reducing the dimensionality of text data, especially in machine learning models."],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","# Sample text data\n","text = \"\"\"\n","ID MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice\n","1 60 RL 65 8450 Pave NA Reg Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story 7 5 2003 2003 Gable CompShg VinylSd VinylSd BrkFace 196 Gd TA PConc Gd TA No GLQ 706 Unf 0 150 856 GasA Ex Y SBrkr 856 854 0 1710 1 0 2 1 3 1 Gd 8 Typ 0 NA Attchd 2003 RFn 2 548 TA TA Y 0 61 0 0 0 0 NA NA NA 0 2 2008 WD Normal 208500\n","2 20 RL 80 9600 Pave NA Reg Lvl AllPub FR2 Gtl Veenker Feedr Norm 1Fam 1Story 6 8 1976 1976 Gable CompShg MetalSd MetalSd None 0 TA TA CBlock Gd TA Gd ALQ 978 Unf 0 284 1262 GasA Ex Y SBrkr 1262 0 0 1262 0 1 2 0 3 1 TA 6 Typ 1 TA Attchd 1976 RFn 2 460 TA TA Y 298 0 0 0 0 0 NA NA NA 0 5 2007 WD Normal 181500\n","3 60 RL 68 11250 Pave NA IR1 Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story 7 5 2001 2002 Gable CompShg VinylSd VinylSd BrkFace 162 Gd TA PConc Gd TA Mn GLQ 486 Unf 0 434 920 GasA Ex Y SBrkr 920 866 0 1786 1 0 2 1 3 1 Gd 6 Typ 1 TA Attchd 2001 RFn 2 608 TA TA Y 0 42 0 0 0 0 NA NA NA 0 9 2008 WD Normal 223500\n","4 70 RL 60 9550 Pave NA IR1 Lvl AllPub Corner Gtl Crawfor Norm Norm 1Fam 2Story 7 5 1915 1970 Gable CompShg Wd Sdng Wd Shng None 0 TA TA BrkTil TA Gd No ALQ 216 Unf 0 540 756 GasA Gd Y SBrkr 961 756 0 1717 1 0 1 0 3 1 Gd 7 Typ 1 Gd Detchd 1998 Unf 3 642 TA TA Y 0 35 272 0 0 0 NA NA NA 0 2 2006 WD Abnorml 140000\n","5 60 RL 84 14260 Pave NA IR1 Lvl AllPub FR2 Gtl NoRidge Norm Norm 1Fam 2Story 8 5 2000 2000 Gable CompShg VinylSd VinylSd BrkFace 350 Gd TA PConc Gd TA Av GLQ 655 Unf 0 490 1145 GasA Ex Y SBrkr 1145 1053 0 2198 1 0 2 1 4 1 Gd 9 Typ 1 TA Attchd 2000 RFn 3 836 TA TA Y 192 84 0 0 0 0 NA NA NA 0 12 2008 WD Normal 250000\n","\"\"\"\n","\n","# Tokenize the text into words\n","words = word_tokenize(text)\n","\n","# Perform POS tagging\n","pos_tags = pos_tag(words)\n","\n","# Print the POS tags\n","print(pos_tags)"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample text data\n","data = {\n","    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    'Text': [\n","        \"RL Pave NA Reg Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story Gable CompShg VinylSd VinylSd BrkFace Gd TA PConc Gd TA No GLQ Unf GasA Ex Y SBrkr GasA TA Y 0 61 0 0 0 0 NA NA NA WD Normal\",\n","        \"RL Pave NA Reg Lvl AllPub FR2 Gtl Veenker Feedr Norm 1Fam 1Story Gable CompShg MetalSd MetalSd None TA TA CBlock Gd TA Gd ALQ Unf GasA Ex Y SBrkr GasA TA Y 298 0 0 0 0 0 NA NA NA WD Normal\",\n","        \"RL Pave NA IR1 Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story Gable CompShg VinylSd VinylSd BrkFace Gd TA PConc Gd TA Mn GLQ Unf GasA Ex Y SBrkr GasA TA Y 0 42 0 0 0 0 NA NA NA WD Normal\",\n","        \"RL Pave NA IR1 Lvl AllPub Corner Gtl Crawfor Norm Norm 1Fam 2Story Gable CompShg Wd Sdng Wd Shng None TA TA BrkTil TA Gd No ALQ Unf GasA Ex Y SBrkr GasA TA Y 0 35 272 0 0 0 NA NA NA WD Abnorml\",\n","        \"RL Pave NA IR1 Lvl AllPub FR2 Gtl NoRidge Norm Norm 1Fam 2Story Gable CompShg VinylSd VinylSd BrkFace Gd TA PConc Gd TA Av GLQ Unf GasA Ex Y SBrkr GasA TA Y 192 84 0 0 0 0 NA NA NA WD Normal\",\n","        \"RL Pave NA IR1 Lvl AllPub Inside Gtl Mitchel Norm Norm 1Fam 1.5Fin Gable CompShg VinylSd VinylSd None TA TA Wood Gd TA No GLQ Unf GasA Ex Y SBrkr GasA TA Y 40 30 0 320 0 0 NA MnPrv Shed\",\n","        \"RL Pave NA Reg Lvl AllPub Inside Gtl Somerst Norm Norm 1Fam 1Story Gable CompShg VinylSd VinylSd Stone Gd TA PConc Ex TA Av GLQ Unf GasA Ex Y SBrkr GasA TA Y 255 57 0 0 0 0 NA NA NA WD Normal\",\n","        \"RL Pave NA IR1 Lvl AllPub Corner Gtl NWAmes PosN Norm 1Fam 2Story Gable CompShg HdBoard HdBoard Stone TA TA CBlock Gd TA Mn ALQ BLQ GasA Ex Y SBrkr GasA TA Y 235 204 228 0 0 0 NA NA Shed\",\n","        \"RM Pave NA Reg Lvl AllPub Inside Gtl OldTown Artery Norm 1Fam 1.5Fin Gable CompShg BrkFace Wd Shng None TA TA BrkTil TA TA No Unf Unf GasA Gd Y FuseF GasA TA Y 90 0 205 0 0 0 NA NA NA WD Abnorml\",\n","        \"RL Pave NA Reg Lvl AllPub Corner Gtl BrkSide Artery Artery 2fmCon 1.5Unf Gable CompShg MetalSd MetalSd None TA TA BrkTil TA TA No GLQ Unf GasA Ex Y SBrkr GasA TA Y 0 4 0 0 0 0 NA NA NA WD Normal\"\n","    ]\n","}\n","\n","# Create a DataFrame\n","df = pd.DataFrame(data)\n","\n","# Initialize CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform the text data\n","X = vectorizer.fit_transform(df['Text'])\n","\n","# Convert the result to DataFrame\n","df_vectorized = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# Concatenate the original DataFrame with the vectorized DataFrame\n","df_concatenated = pd.concat([df, df_vectorized], axis=1)\n","\n","# Display the result\n","print(df_concatenated)\n"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["I used the CountVectorizer for text vectorization. Here's why:\n","\n","CountVectorizer:\n","\n","It converts a collection of text documents into a matrix of token counts, where each row represents a document and each column represents a unique word in the corpus.\n","It counts the frequency of each word in the document, which can be a useful feature for various machine learning algorithms.\n","It is simple and easy to understand, making it a good choice for basic text analysis tasks.\n","Why CountVectorizer:\n","\n","In this scenario, the goal seems to be to convert the text data into a format suitable for machine learning algorithms, where each word's frequency serves as a feature.\n","CountVectorizer provides a straightforward way to achieve this by converting text data into a sparse matrix representation."],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","data = pd.read_csv(\"/content/cleaned_data.csv\")\n","\n","# Drop columns with high correlation\n","high_corr_cols = ['GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd']\n","data.drop(high_corr_cols, axis=1, inplace=True)\n","\n","# Create new feature 'TotalSF' by adding 'TotalBsmtSF', '1stFlrSF', and '2ndFlrSF'\n","data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n","\n","# Create new feature 'TotalBath' by adding 'FullBath' and 'HalfBath'\n","data['TotalBath'] = data['FullBath'] + 0.5 * data['HalfBath']\n","\n","# Create new feature 'TotalPorchSF' by adding 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', and 'ScreenPorch'\n","data['TotalPorchSF'] = data['OpenPorchSF'] + data['EnclosedPorch'] + data['3SsnPorch'] + data['ScreenPorch']\n","\n","# Drop original columns used to create new features\n","data.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'FullBath', 'HalfBath', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'], axis=1, inplace=True)\n","\n","# Check the modified dataset\n","print(data.head())\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Lasso\n","\n","# Load the dataset\n","data = pd.read_csv(\"/content/cleaned_data.csv\")\n","\n","# Drop irrelevant columns (e.g., ID)\n","data.drop(\"Id\", axis=1, inplace=True)\n","\n","# Handle missing values (e.g., fill with mean or mode)\n","data.fillna(data.mean(), inplace=True)\n","\n","# Encode categorical variables (e.g., one-hot encoding)\n","data = pd.get_dummies(data)\n","\n","# Split data into features and target variable\n","X = data.drop(\"SalePrice\", axis=1)\n","y = data[\"SalePrice\"]\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize and fit Lasso regression model\n","lasso = Lasso(alpha=0.1)  # Adjust alpha as needed\n","lasso.fit(X_train_scaled, y_train)\n","\n","# Evaluate model\n","train_score = lasso.score(X_train_scaled, y_train)\n","test_score = lasso.score(X_test_scaled, y_test)\n","\n","print(\"Train R^2 score:\", train_score)\n","print(\"Test R^2 score:\", test_score)\n"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Correlation Analysis: This method measures the linear relationship between features and the target variable. Features with high correlation coefficients (either positive or negative) with the target are considered important. It helps identify features that have a strong influence on the target variable.\n","\n","Feature Importance from Tree-based Models: Tree-based models like Random Forest and Gradient Boosting Machines provide a feature importance score based on how frequently a feature is used to split the data across all the trees in the ensemble. Features with higher importance scores are considered more important for prediction.\n","\n","Recursive Feature Elimination (RFE): RFE recursively removes features and builds a model on the remaining features until the specified number of features is reached. It ranks features based on their importance and eliminates the least important ones. RFE helps in selecting the most relevant features for the model."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["OverallQual: The overall quality rating of the house is often a strong predictor of its price. Higher-quality houses tend to have higher prices.\n","\n","GrLivArea: The above-ground living area (in square feet) is a crucial factor in determining the price of a house. Larger living areas often correlate with higher prices.\n","\n","TotalBsmtSF: The total basement area (in square feet) is another significant feature. A larger basement area can add value to a house.\n","\n","YearBuilt: The year the house was built can influence its price. Newer houses may be priced higher due to modern amenities and construction standards.\n","\n","Neighborhood: The neighborhood in which the house is located can have a significant impact on its price. Desirable neighborhoods with good schools, amenities, and low crime rates tend to command higher prices.\n","\n","GarageCars and GarageArea: The size and capacity of the garage can affect the price of a house. More garage space or higher capacity for cars can increase the value."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# Load the data\n","data = pd.read_csv(\"/content/cleaned_data.csv\")\n","\n","# Separate features and target variable\n","X = data.drop(columns=[\"SalePrice\"])  # Features\n","y = data[\"SalePrice\"]  # Target variable\n","\n","# Define numerical and categorical features\n","numerical_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n","categorical_features = X.select_dtypes(include=[\"object\"]).columns\n","\n","# Define preprocessing steps for numerical and categorical features\n","numerical_transformer = Pipeline(steps=[\n","    (\"imputer\", SimpleImputer(strategy=\"median\")),\n","    (\"scaler\", StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n","    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n","])\n","\n","# Combine preprocessing steps\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        (\"num\", numerical_transformer, numerical_features),\n","        (\"cat\", categorical_transformer, categorical_features)\n","    ])\n","\n","# Apply preprocessing to the data\n","X_transformed = preprocessor.fit_transform(X)\n","\n","# Now X_transformed contains the transformed features\n"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the data\n","data = pd.read_csv(\"/content/cleaned_data.csv\")\n","\n","# Separate features and target variable\n","X = data.drop(columns=[\"SalePrice\"])  # Features\n","y = data[\"SalePrice\"]  # Target variable\n","\n","# Select numerical features\n","numerical_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n","\n","# Scale numerical features\n","scaler = StandardScaler()\n","X[numerical_features] = scaler.fit_transform(X[numerical_features])\n","\n","# Now X contains the scaled numerical features\n"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?\n","\n","> Indented block\n","\n"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Whether dimensionality reduction is needed depends on various factors such as the nature of the dataset, the machine learning algorithm being used, computational resources available, and the desired outcome.\n","\n","Here are some reasons why dimensionality reduction might be needed:\n","\n","Curse of Dimensionality: As the number of features increases, the volume of the feature space grows exponentially, which can lead to sparsity of data points. This can make it difficult for machine learning algorithms to effectively learn from the data, leading to overfitting or poor generalization.\n","\n","Computational Efficiency: High-dimensional datasets require more computational resources (memory and processing power) for training machine learning models. Dimensionality reduction can help reduce the computational burden by simplifying the dataset while preserving most of the important information.\n","\n","Visualization: It is challenging to visualize data in high-dimensional spaces. Dimensionality reduction techniques project the data onto lower-dimensional spaces, making it easier to visualize and interpret.\n","\n","Noise Reduction: Dimensionality reduction techniques can help remove redundant or noisy features, leading to better performance and more interpretable models.\n","\n","Model Performance: In some cases, reducing the dimensionality of the dataset can improve the performance of machine learning models by focusing on the most informative features and reducing the impact of irrelevant or redundant ones.\n","\n","However, dimensionality reduction is not always necessary or beneficial. Here are some reasons why it might not be needed:\n","\n","Informative Features: If the dataset contains a small number of highly informative features, reducing dimensionality may result in loss of important information and degrade model performance.\n","\n","Interpretability: In some cases, maintaining the original features is important for interpretability, especially if the goal is to understand the relationships between input features and the target variable.\n","\n","Algorithm Compatibility: Some machine learning algorithms, such as tree-based models like Random Forests or Gradient Boosting Machines, can handle high-dimensional data efficiently without the need for dimensionality reduction."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the dataset\n","data = pd.read_csv(\"/content/cleaned_data.csv\")\n","\n","# Handle missing values and convert categorical variables if needed\n","\n","# Select features and target variable\n","X = data.drop(columns=[\"Id\", \"SalePrice\"])  # Features\n","y = data[\"SalePrice\"]  # Target variable\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(n_components=0.95)  # Retain 95% of variance\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# Optional: Print the explained variance ratio\n","print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n","\n","# Train machine learning models using X_pca and evaluate their performance\n","# For example:\n","# Split the data into train and test sets\n","# Train your models (e.g., Linear Regression, Random Forest) using X_pca\n","# Evaluate the models' performance on the test set\n"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["Preservation of Variance: PCA identifies the directions (principal components) that maximize the variance in the data. By retaining components that capture the most variance, we can reduce the dimensionality of the dataset while still preserving much of its variability.\n","\n","Orthogonality: PCA ensures that the principal components (new features) are orthogonal to each other. This orthogonality property simplifies interpretation and reduces multicollinearity among features, which can be beneficial for various machine learning algorithms.\n","\n","Computational Efficiency: PCA is computationally efficient, making it suitable for large datasets with many features. It accomplishes dimensionality reduction by performing eigenvalue decomposition on the covariance matrix of the standardized data, which can be efficiently computed using techniques like Singular Value Decomposition (SVD).\n","\n","Simplicity: PCA is conceptually simple and easy to implement. It's a linear transformation technique that doesn't require complex parameter tuning, making it accessible for practitioners and suitable for exploratory data analysis.\n","\n","Interpretability: While the original features may not be directly interpretable in PCA-transformed space, the principal components themselves represent combinations of the original features. This can provide insights into which features contribute most to the variability in the dataset."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Assuming 'data' contains your dataset\n","# Separate features (X) and target variable (y)\n","X = data.drop(columns=['Id', 'SalePrice'])  # Assuming 'ID' and 'SalePrice' are not features\n","y = data['SalePrice']\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Now you have X_train (features for training), X_test (features for testing),\n","# y_train (target variable for training), and y_test (target variable for testing)\n"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["The reason for choosing this ratio is that it's a commonly used practice in machine learning for small to moderate-sized datasets. The majority of the data (80%) is used for training the model, allowing it to learn patterns and relationships within the data. The remaining portion (20%) is reserved for testing the model's performance on unseen data, helping to evaluate its generalization ability and detect overfitting.\n","\n","However, the choice of the splitting ratio can depend on various factors, including the size of the dataset, the complexity of the problem, and the availability of data."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why.\n","\n","> Indented block\n","\n"],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Typically, imbalanced datasets refer to classification problems where the distribution of classes is skewed, meaning one class significantly outnumbers the others. This can pose challenges for machine learning models as they may become biased towards the majority class and perform poorly on the minority class.\n","\n"],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["from imblearn.over_sampling import RandomOverSampler\n","import pandas as pd\n","\n","# Assuming 'data' is your DataFrame containing the dataset\n","# Assuming 'target_column' is the name of the target variable column\n","\n","# Separate features and target variable\n","X = data.drop(columns=['SalePrice'])  # Assuming 'SalePrice' is the target variable\n","y = data['SalePrice']\n","\n","# Initialize RandomOverSampler\n","oversampler = RandomOverSampler()\n","\n","# Perform oversampling\n","X_resampled, y_resampled = oversampler.fit_resample(X, y)\n","\n","# Convert X_resampled and y_resampled back to DataFrame if needed\n","X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n","y_resampled_df = pd.Series(y_resampled, name='SalePrice')\n","\n","# Now you can use X_resampled_df and y_resampled_df for training your model\n"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["I used the RandomOverSampler technique from the imbalanced-learn library to handle the imbalanced dataset. This technique was chosen because it helps balance the class distribution by randomly oversampling the minority class instances until the class distribution is approximately balanced. Random oversampling creates duplicate instances of the minority class, which helps prevent the model from being biased towards the majority class during training.\n","\n"],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Assuming X contains features and y contains target variable\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Random Forest Regressor model\n","model = RandomForestRegressor()\n","\n","# Fit the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"Mean Squared Error:\", mse)\n"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Assuming you have a dictionary of MSE values for each model\n","mse_values = {\n","    'Model 1': mse1,\n","    'Model 2': mse2,\n","    'Model 3': mse3,\n","    'Model 4': mse4\n","}\n","\n","# Plotting the evaluation metric score chart\n","plt.figure(figsize=(10, 6))\n","plt.bar(mse_values.keys(), mse_values.values(), color='skyblue')\n","plt.xlabel('Models')\n","plt.ylabel('Mean Squared Error (MSE)')\n","plt.title('Evaluation Metric Score Chart')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Load and preprocess your dataset\n","# Assuming you have loaded your dataset into X and y variables\n","\n","# Split your data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define your model\n","model = RandomForestRegressor()\n","\n","# Define the hyperparameters grid for GridSearch CV or RandomSearch CV\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","    'bootstrap': [True, False]\n","}\n","\n","# Perform GridSearch CV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best hyperparameters and the best model\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","# Predict on the test set using the best model\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate the model\n","# You can use appropriate evaluation metrics (e.g., RMSE, MAE, R^2)\n"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["Write the conclusion here."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}